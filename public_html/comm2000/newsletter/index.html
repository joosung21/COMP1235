<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
      Is AI Fair? The Hidden Bias in Algorithms That Could Affect You
    </title>
    <link
      href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700;900&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Oswald:wght@700&display=swap"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <div class="wrapper">
      <!-- Top anchor for back-to-top links -->
      <div
        style="
          padding: 20px 20px;
          display: flex;
          justify-content: space-between;
        "
      >
        <img src="images/logo.png" alt="logo" width="200" />
        <div style="opacity: 70%">Newsletter 101539659</div>
      </div>
      <!-- Header with background cover image -->
      <header>
        <div class="header-content">
          <div
            style="
              background-color: black;
              color: white;
              padding: 12px;
              display: inline-block;
              font-weight: 900;
              font-family: 'Oswald', sans-serif;
              letter-spacing: 1px;
              text-transform: uppercase;
            "
          >
            Algorithms Bias
          </div>
          <div style="background: white; padding: 20px">
            <h1>
              Is AI Fair? The Hidden Bias in Algorithms That Could Affect You
            </h1>
            <p class="tagline">
              Exploring the social impact of algorithmic bias in artificial
              intelligence.
            </p>
            <p class="tagline" style="opacity: 35%">by Joosung Ahn</p>
          </div>
        </div>
      </header>

      <!-- Table of Contents -->
      <nav class="table-of-contents">
        <div id="top"></div>
        <h2>Table of Contents</h2>
        <ul>
          <li>
            <a href="#introduction"
              >Is AI Fair? The Hidden Bias in Algorithms That Could Affect
              You</a
            >
          </li>
          <li><a href="#problem">The Problem: Why AI Bias Matters</a></li>
          <li>
            <a href="#healthcare"
              >When AI Gets It Wrong: Racial Bias in Healthcare</a
            >
          </li>
          <li>
            <a href="#justice">The AI That Decides Your Future in Court</a>
          </li>
          <li>
            <a href="#lending"
              >Who Gets a Loan? The Hidden Bias in AI Lending</a
            >
          </li>
          <li><a href="#action">Take Action: How You Can Help</a></li>
        </ul>
      </nav>

      <!-- Main content -->
      <main class="container">
        <!-- Introduction Section -->
        <section id="introduction">
          <img src="images/pic1.jpg" alt="AI" class="featured-image" />
          <h2>
            Is AI Fair? The Hidden Bias in Algorithms That Could Affect You
          </h2>
          <p>
            Artificial intelligence (AI) is rapidly transforming the way we
            live, work, and interact, promising innovation and unprecedented
            efficiency. Yet, beneath its dazzling potential lies a critical flaw
            that many have overlooked—algorithmic bias. At its core, algorithmic
            bias occurs when AI systems produce prejudiced results due to flawed
            data sets or biased programming assumptions. This bias is not merely
            a technical error; it has deep social implications that can affect
            healthcare, judicial decisions, and financial services.
          </p>
          <p>
            In this newsletter, we explore how such biases infiltrate AI systems
            and ultimately impact the lives of everyday people in Toronto and
            beyond. Drawing on a range of case studies and research—including
            analyses from
            <a
              href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2778569#google_vignette"
              target="_blank"
              >algorithmovigilance study</a
            >
            in healthcare and investigations into judicial AI tools such as
            <a
              href="https://link.springer.com/article/10.1007/s10506-024-09389-8"
              target="_blank"
              >COMPAS article</a
            >, we delve into real-world examples of bias that contribute to
            unequal treatment. With voices from academic experts and local
            advocates, we make the case that addressing AI bias is not only
            necessary but urgent for building a fairer society.
          </p>
          <a class="back-to-top" href="#top">Back to Top</a>
        </section>

        <!-- The Problem Section -->
        <section id="problem">
          <img src="images/pic2.jpg" alt="AI" class="featured-image" />
          <h2>The Problem: Why AI Bias Matters</h2>
          <p>
            Bias in AI systems is emerging as one of the most pressing ethical
            issues of our time. When algorithms are fed with skewed data or
            inherit the prejudices of their creators, the results can be deeply
            problematic. For example, biased data sets can lead to
            discriminatory outcomes in risk assessments, job recruitment, and
            even criminal sentencing. The ramifications extend far beyond
            isolated technical errors—they affect human lives, contribute to
            social inequities, and perpetuate systemic injustices.
          </p>
          <p>
            Recent studies, such as those covered by investigative reports on AI
            auditing (like
            <a
              href="https://www.media.mit.edu/publications/actionable-auditing-investigating-the-impact-of-publicly-naming-biased-performance-results-of-commercial-ai-products/"
              target="_blank"
              >auditing research</a
            >), reveal how public naming of biased performance in commercial AI
            products has pressured companies to rethink their methodologies.
            Similarly, warnings from local authorities like the Ontario Privacy
            Commissioner have highlighted the “Wild West” nature of unregulated
            AI use (
            <a
              href="https://financialpost.com/pmn/business-pmn/ontario-privacy-commissioner-feels-urgency-to-address-wild-west-risks-of-ai-2"
              target="_blank"
              >local concerns</a
            >). With Toronto’s diverse population, ensuring fairness in AI is
            not just an academic concern—it is a community imperative that
            impacts every resident.
          </p>
          <a class="back-to-top" href="#top">Back to Top</a>
        </section>

        <!-- Healthcare Section -->
        <section id="healthcare">
          <img src="images/pic3.jpg" alt="AI" class="featured-image" />
          <h2>When AI Gets It Wrong: Racial Bias in Healthcare</h2>
          <p>
            One of the most alarming examples of algorithmic bias is found in
            healthcare. AI systems are increasingly used to predict patient risk
            scores and recommend treatments. However, research has shown that
            some of these systems may undervalue the health risks of patients
            from marginalized communities. For instance, studies under the
            “Algorithmovigilance” framework have documented that biased data can
            lead to lower risk scores for Black patients, ultimately limiting
            their access to essential healthcare services. Meanwhile,
            <a
              href="https://www.thelancet.com/journals/landig/article/PIIS2589-7500(20)30065-0/fulltext"
              target="_blank"
              >Lancet article</a
            >
            points out how gender and racial biases can inadvertently become
            embedded in healthcare algorithms.
          </p>
          <p>
            The implications are significant: when AI miscalculates a patient’s
            needs, it not only undermines trust in the healthcare system but can
            also lead to life-threatening consequences. As healthcare providers
            and tech developers work to refine these systems, there is a
            pressing need for transparent auditing and ethical oversight. The
            situation calls for an immediate reassessment of the data and
            algorithms that shape these critical decisions.
          </p>
          <a class="back-to-top" href="#top">Back to Top</a>
        </section>

        <!-- Justice Section -->
        <section id="justice">
          <img src="images/pic4.jpg" alt="AI" class="featured-image" />
          <h2>The AI That Decides Your Future in Court</h2>
          <p>
            In the realm of criminal justice, AI tools such as the widely
            debated COMPAS system are used to predict recidivism risks.
            Unfortunately, these systems are not immune to bias. Multiple
            investigations have revealed that the algorithms can
            disproportionately label minority defendants as high risk, thereby
            influencing bail decisions, sentencing, and parole. This raises
            profound questions about fairness and accountability in legal
            proceedings.
          </p>
          <p>
            Critics argue that relying on opaque algorithms to determine
            life-altering judicial outcomes is dangerous. The notion that a
            computer program could potentially dictate an individual’s future
            undermines the principles of justice and equal treatment under the
            law. With the stakes so high, reform efforts are underway, with
            advocates calling for more rigorous testing and transparent
            methodologies in AI-driven legal tools. Locally, researchers at
            several institutions—including
            <a
              href="https://www.utoronto.ca/news/u-t-researcher-launches-group-help-detect-hidden-biases-ai-systems"
              target="_blank"
              >U of T team</a
            >—are examining how to reduce bias in legal AI applications.
          </p>
          <a class="back-to-top" href="#top">Back to Top</a>
        </section>

        <!-- Lending Section -->
        <section id="lending">
          <img src="images/pic0.jpg" alt="AI" class="featured-image" />
          <h2>Who Gets a Loan? The Hidden Bias in AI Lending</h2>
          <p>
            Another area where algorithmic bias is making headlines is in the
            field of mortgage lending and financial services. Many financial
            institutions have turned to AI-driven models to assess
            creditworthiness and determine loan eligibility. However, evidence
            from
            <a
              href="https://www.media.mit.edu/publications/actionable-auditing-investigating-the-impact-of-publicly-naming-biased-performance-results-of-commercial-ai-products/"
              target="_blank"
              >MIT Media Lab publication</a
            >
            suggests that these models can systematically disadvantage minority
            applicants by using biased historical data as a benchmark.
          </p>
          <p>
            Such discriminatory practices have long-term consequences: families
            may be denied the opportunity to purchase homes or secure favorable
            loans, further entrenching economic disparities. As financial
            institutions begin to acknowledge these biases, there is a growing
            call for more equitable models that factor in diverse socioeconomic
            backgrounds. This shift is crucial for creating a financial system
            that serves all members of our community fairly.
          </p>
          <a class="back-to-top" href="#top">Back to Top</a>
        </section>

        <!-- Take Action Section -->
        <section id="action">
          <img src="images/pic5.jpg" alt="AI" class="featured-image" />
          <h2>Take Action: How You Can Help</h2>
          <p>
            Addressing algorithmic bias is a collective effort that begins with
            awareness and extends to active participation. Whether you are a
            student, educator, or tech professional, there are several ways you
            can contribute to the fight for fair AI:
          </p>
          <ul>
            <li>
              Engage in local forums and discussions about ethical AI
              development.
            </li>
            <li>
              Support initiatives and research projects that advocate for
              transparent and accountable AI.
            </li>
            <li>
              Stay informed by reading up on the latest case studies and
              regulatory updates in AI ethics.
            </li>
            <li>
              Participate in workshops and training sessions that focus on
              reducing bias in technology.
            </li>
          </ul>
          <p>
            By taking these steps, you can help build a community that demands
            and drives change. As technology becomes increasingly woven into the
            fabric of our lives, ensuring that AI serves everyone fairly is not
            only a professional responsibility—it is a moral imperative.
            Together, we can challenge the status quo and push for AI systems
            that are as inclusive as they are innovative.
          </p>
          <div style="display: flex; justify-content: center; margin-top: 20px">
            <a
              class="cta"
              style="
                border: 1px solid red;
                padding: 10px 20px;
                border-radius: 5px;
                background-color: rgb(210, 51, 51);
                color: white;
                font-weight: 700;
                text-decoration: none;
              "
              href="#"
              onClick="alert('Thank you for your support!')"
            >
              <strong>Take Action Against AI Bias – Join the Movement!</strong>
            </a>
          </div>
          <div style="text-align: center">
            <a class="back-to-top" href="#top">Back to Top</a>
          </div>
        </section>
      </main>

      <!-- Footer -->
      <footer>
        <p>&copy; 2025 The Local. All rights reserved.</p>
      </footer>
    </div>
  </body>
</html>
